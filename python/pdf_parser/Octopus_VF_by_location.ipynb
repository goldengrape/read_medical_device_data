{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取Humphrey视野数据\n",
    "\n",
    "使用新的方式重构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 设定文件路径参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    input_path='../../testdata/o2'\n",
    "    output_path=\"../../testdata/o2\"\n",
    "    fname=\"dec_83\\303\\317\\225F20131106\\266\\257\\314\\254\\312\\323\\322\\260(Octopus) .pdf\"\n",
    "\n",
    "    info_location_path='../../medical_device_data/'\n",
    "    info_basic_fname=\"octopus_basic_location.csv\"\n",
    "    info_LVC_fname=\"octopus_LVC_location.csv\"\n",
    "    info_G_fname=\"octopus_G_location.csv\"\n",
    "    info_fname_dict={\"basic\": info_basic_fname, \"LVC\":info_LVC_fname, \"G\":info_G_fname}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入依赖包"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在使用notebook.azure.com在线运行时, 由于默认没有安装pdfminer.six这个包, 所以在首次运行时需要安装, 已经将安装代码加入到下面导入依赖包的代码内, 因此首次运行时速度会较慢. \n",
    "\n",
    "同时, 在使用notebook.azure.com在线运行时, 服务器端不会保存曾经安装过的包, 因此在1小时没有操作之后, 服务器会关闭, 再次打开时就已经丢失了之前安装的包, 相当于首次运行. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "import io\n",
    "import re\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "try:\n",
    "    from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "    from pdfminer.pdfpage import PDFPage\n",
    "    from pdfminer.converter import XMLConverter, HTMLConverter, TextConverter\n",
    "    from pdfminer.layout import LAParams\n",
    "except:\n",
    "    !conda install pdfminer.six --yes\n",
    "    from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "    from pdfminer.pdfpage import PDFPage\n",
    "    from pdfminer.converter import XMLConverter, HTMLConverter, TextConverter\n",
    "    from pdfminer.layout import LAParams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入 PDF_parser_by_location \n",
    "PDF_parser_by_location 中将所有PDF转换成带有html, 其中每个字符均有定位, 通过选取一个方框来对一个数据或者单词进行选择. 各个数据的定位数据放置在相应的csv文件中, 由info_lation_path和info_fname保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PDF_parser_by_location import read_data_from_location, pdf_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_page(input_path,fname):\n",
    "    '''\n",
    "    取得页面个数\n",
    "    '''\n",
    "    filename=os.path.join(input_path,fname)\n",
    "    fp = open(filename, 'rb')\n",
    "    return len([p for p in PDFPage.get_pages(fp)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_one_data(input_path, fname, info_location_path, info_fname,page_number):\n",
    "    '''\n",
    "    处理一页内容\n",
    "    '''\n",
    "    df=read_data_from_location(input_path, fname, info_location_path, info_fname, page_number)\n",
    "    df=df.set_index(\"item_name\")\n",
    "    return df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_method(input_path, fname, info_location_path, info_fname_dict, page_number):\n",
    "    df=read_one_data(input_path, fname, info_location_path, info_fname_dict[\"basic\"],page_number)\n",
    "    return df.Programs.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_data(input_path, fname, info_location_path, info_fname_dict, page_number):\n",
    "    test_method=get_test_method(input_path, fname, info_location_path, info_fname_dict,page_number)\n",
    "    for key in info_fname_dict.keys():\n",
    "        if key in test_method: \n",
    "            info_fname=info_fname_dict[key]\n",
    "    df=read_one_data(input_path, fname, info_location_path, info_fname, page_number)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 清洗数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    df[\"Name\"]=df[\"name and birthday\"].str.extract('([\\s\\S]+),').astype(\"str\")\n",
    "    df[\"Birthday\"]=df[\"name and birthday\"].str.extract('(\\d+-\\d+-\\d+)').astype(\"str\")\n",
    "    df.drop(\"name and birthday\", axis=1, inplace=True)\n",
    "    \n",
    "    df[\"Eye\"]=df[\"Eye and exam date time\"].str.extract('(O[D|S])').astype(\"str\")\n",
    "    df[\"Date\"]=df[\"Eye and exam date time\"].str.extract('(\\d+-\\d+-\\d+)').astype(\"str\")\n",
    "    df[\"Time\"]=df[\"Eye and exam date time\"].str.extract('(\\d+:\\d+:\\d+)').astype(\"str\")\n",
    "    df.drop(\"Eye and exam date time\", axis=1, inplace=True)\n",
    "    df[\"Refraction\"]=df[\"Refraction\"].astype(\"str\")\n",
    "    \n",
    "    # re-order\n",
    "    cols = df.columns.tolist()\n",
    "    \n",
    "    \n",
    "#     df[\"patient\"]=df[\"patient\"].str.replace(\",\",\"\")\n",
    "#     df[\"date of birth\"]=pd.to_datetime(df[\"date of birth\"])\n",
    "#     df[\"gender\"]=(df[\"gender\"]\n",
    "#                   .str.replace(\"其他\",\"Other\")\n",
    "#                   .str.replace(\"女性\",\"Female\")\n",
    "#                   .str.replace(\"男性\",\"Male\")\n",
    "#                  )\n",
    "#     df[\"Date\"]=pd.to_datetime(df[\"Date\"])\n",
    "#     # Fixation Losses不知为何有可能在excel里被解析成日期, 但csv以纯文本打开不会\n",
    "#     df[\"Fixation Losses\"]=df[\"Fixation Losses\"].str.extract('(\\d+\\/\\d+)').astype(\"str\")\n",
    "#     df[\"False POS Errors\"]=df[\"False POS Errors\"].str.extract('(\\d+\\%)').astype(\"str\") \n",
    "#     df[\"False NEG Errors\"]=df[\"False NEG Errors\"].str.extract('(\\d+\\%)').astype(\"str\")\n",
    "#     df[\"Background\"]=df[\"Background\"].str.extract('(\\-{,1}\\d+\\.{,1}\\d*)').astype(\"float\")\n",
    "#     df[\"Pupil Diameter\"]=df[\"Pupil Diameter\"].str.extract('(\\-{,1}\\d+\\.{,1}\\d*)').astype(\"float\")\n",
    "    \n",
    "#     # 将字符串转换为数字\n",
    "#     # 带有最多一个负号, 跟至少一个数字, 带有最多一个小数点, 小数点后有或者没有数字\n",
    "#     for col in df.iloc[:,24:]:\n",
    "#         df[col]=df[col].str.extract('(\\-{,1}\\d+\\.{,1}\\d*)').astype(\"float\") \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理单个文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deal_with_one_file(input_path, fname, info_location_path, info_fname_dict):\n",
    "    pages=get_pdf_page(input_path,fname)\n",
    "    df=DataFrame()\n",
    "    for page_number in range(pages):\n",
    "        newdf=get_full_data(input_path, fname, info_location_path, info_fname_dict,page_number)\n",
    "        newdf=clean_data(newdf)\n",
    "        df=df.append(newdf, sort=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deal_with_folder(input_path, fname, info_location_path, info_fname_dict):\n",
    "    pdffiles = [name for name in os.listdir(input_path)\n",
    "            if name.endswith('.pdf')]\n",
    "    df=DataFrame()\n",
    "    N=len(pdffiles)\n",
    "    i=0\n",
    "    start_time = timeit.default_timer()\n",
    "    for fname in pdffiles:\n",
    "        newdf=deal_with_one_file(input_path, fname, info_location_path, info_fname_dict)\n",
    "        df=df.append(newdf, sort=False)\n",
    "        print(os.path.join(input_path,fname)+\" Done!\")\n",
    "        elapsed = timeit.default_timer() - start_time\n",
    "        i+=1\n",
    "        print(str(int(i/N*100))+\"%\")\n",
    "        print(\"each file time ~={}sec\".format(int(elapsed/i)))\n",
    "        print(\"total time ~={}sec\".format(int(elapsed/i*N)))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../testdata/o2/dec_272\\313\\325\\277\\241\\215H20161013\\266\\257\\314\\254\\312\\323\\322\\260(Octopus) .pdf Done!\n",
      "33%\n",
      "each file time ~=5sec\n",
      "total time ~=15sec\n",
      "../../testdata/o2/dec_272\\313\\325\\277\\241\\215H20161226\\266\\257\\314\\254\\312\\323\\322\\260(Octopus) .pdf Done!\n",
      "66%\n",
      "each file time ~=5sec\n",
      "total time ~=16sec\n",
      "../../testdata/o2/dec_272\\313\\325\\277\\241\\215H20171030\\266\\257\\314\\254\\312\\323\\322\\260(Octopus) .pdf Done!\n",
      "100%\n",
      "each file time ~=5sec\n",
      "total time ~=16sec\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    df=deal_with_folder(input_path, fname, info_location_path, info_fname_dict)\n",
    "    df.to_csv(os.path.join(output_path,\"Octopus_data.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
